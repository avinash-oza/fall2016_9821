\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework 1}
\newcommand{\hmwkDueDate}{February 12, 2016}
\newcommand{\hmwkClass}{Stochastic Calculus}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{Professor Anja Richter}
\newcommand{\hmwkAuthorName}{Xinlu Xiao}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\pagebreak




\begin{homeworkProblem}

Chapter 6 Problem 3 Finished by Xinlu Xiao\\
(i)\[
A = \left(
\begin{matrix}
1 & -0.2 & 0.8\\
-0.2 & 2 & 0.3\\
0.8 & 0.3 & 1.1
\end{matrix}\right)
\]
 $R_{1} = \sum_{k = 2:3} |A(1,k)|$,  so $R_{1} = 0.2 + 0.8 = 1$, and $|A(1,1)| = 1$, we know that $|A(1,1)| = R_{1}$.\\
$R_{2} = \sum_{k = 1, 3} |A(2,k)|$,  so $R_{2} = 0.2 + 0.3 = 0.5$, and $|A(2,2)| = 2$, we know that $|A(2,2)| > R_{2}$.\\
$R_{3} = \sum_{k = 1, 2} |A(3,k)|$,  so $R_{3} = 0.8 + 0.3 = 1.1$, and $|A(3,3)| = 1.1$, we know that $|A(3,3)| = R_{3}$.\\
So by the definition of weakly diagonal dominant matrix, we have $|A(j,j)| \geq R_{j}, \forall j = 1:3$, so matrix $A$ is weakly diagonal dominant matrix.\\
And from the Gershgorin's Theroem, we know that $|A(j,j)-\lambda| \leq R_{j}$, so we can get:
\[
A(j,j) - \lambda \leq |A(j,j)-\lambda| \leq R_{j} \]
and therefore,
\[
\lambda \geq A(j,j) - R_{j}\]
And since $A$ is a weakly diagonal dominant matrix, so that $A(j,j) \geq R_{j}$, then we can conclude that $\lambda \geq 0$ for any eigenvalue $\lambda$ of A, and therefore that the matrix A is symmetric positive definite.\\
\\
(ii) We can calculate all the leading minors of the matrix $A$:
\[
det\left(
\begin{matrix}
1
\end{matrix}\right) = 1 > 0
\]
\[
det\left(
\begin{matrix}
1 & -0.2\\
-0.2 & 2
\end{matrix}\right) = 1.96>0 \]
\[
det\left(
\begin{matrix}
1 & -0.2 & 0.8\\
-0.2 & 2 & 0.3\\
0.8 & 0.3 & 1.1
\end{matrix}\right) = 0.69 > 0\]
So that all the leading principal minors are positive, according to Sylvester's Criterion, we know that the matrix $A$ is symmetric positive definite.\\

\end{homeworkProblem}





\begin{homeworkProblem}

Chapter 6 Problem 6 Finished by Xinlu Xiao\\
\[
B_{N} = \left(
\begin{matrix}
2 & -1 & \dots & 0 \\
-1 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & -1\\
0 & \dots & -1 & 2
\end{matrix} \right) = U_{N}^{t}U_{N}\]
\[= 
\left(
\begin{matrix}
U_{N}(1,1) & 0 & 0 & \dots & 0\\
U_{N}(1,2) & U_{N}(2,2) & \dots & \dots & 0\\
\vdots & \ddots & \ddots & \ddots & \vdots\\
U_{N}(1,N) & \dots & \dots & \dots & U_{N}(N,N)
\end{matrix} \right) 
\left(
\begin{matrix}
U_{N}(1,1) & U_{N}(1,2) & \dots & \dots & U_{N}(1,N)\\
0 & U_{N}(2,2) & \dots & \dots & U_{N}(2,N)\\
\vdots & \ddots & \ddots & \ddots & \vdots\\
0 & \dots & \dots & \dots & U_{N}(N,N)
\end{matrix}\right)
\]
From the multiplication of the matrix, we know that 
\[
U_{N}(1,1)^2 = B_{N}(1,1), \quad so \quad U_{N}(1,1) = \sqrt{2}\]
\[
U_{N}(1,1)U_{N}(1,1:N) = B_{N}(1,1:N)\]
And since $B_{N}$ is a triangular symmetric positive definite matrix, we know that $B_{N}(1,3:N) = \emph{\textbf{0}}$, and since $U_{N}(1,1) \neq 0$, so that $U_{N}(1,3:N) = \emph{\textbf{0}}$.\\
And 
\[
U_{N}(1,1)U_{N}(1,2) = B_{N}(1,2), so \quad U_{N}(1,2) = \frac{B_{N}(1,2)}{U_{N}(1,1)} = -\frac{1}{\sqrt{2}}\]
then we update the matrix $B_{N}$
\[
B_{N}(2:N,2:N) = B_{N}(2:N,2:N)-(U_{N}(1,2:N))^{t}U(1,2:N) \]
\[
=
\left(
\begin{matrix}
2 & -1 & \dots & 0 \\
-1 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & -1\\
0 & \dots & -1 & 2
\end{matrix} \right) - 
\left(
\begin{matrix}
-\frac{1}{\sqrt{2}}\\
0\\
\dots\\
0
\end{matrix} \right)
\left(
\begin{matrix}
 -\frac{1}{\sqrt{2}}&0&\dots&0
\end{matrix}\right)=
\left(
\begin{matrix}
\frac{3}{2} & -1 & \dots & 0 \\
-1 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & -1\\
0 & \dots & -1 & 2
\end{matrix} \right)\]
And similarly, we can get the
\[
U_{N}(2,2) = \sqrt{\frac{3}{2}}, and \quad U_{N}(2,3) = \frac{B_{N}(2,3)}{U_{N}(2,2)} = -\frac{1}{\sqrt{\frac{3}{2}}} = -\sqrt{\frac{2}{3}},\quad and \quad U_{N}(2,4:N) =\emph{\textbf{0}} \]
And we can use the induction, assume that $U_{N}(i,i) = \sqrt{\frac{i+1}{i}}, \forall i=1:N$ and $U_{N}(i,i+1) = -\sqrt{\frac{i}{i+1}}, \forall i=1:N-1$ 
\[
B_{N}((i+1):N,(i+1):N) = B_{N}((i+1):N,(i+1):N)-(U_{N}(i,(i+1):N))^{t}U(i,(i+1):N) \]
\[
=
\left(
\begin{matrix}
2 & -1 & \dots & 0 \\
-1 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & -1\\
0 & \dots & -1 & 2
\end{matrix} \right) - 
\left(
\begin{matrix}
-\sqrt{\frac{i}{i+1}}\\
0\\
\dots\\
0
\end{matrix} \right)
\left(
\begin{matrix}
 -\sqrt{\frac{i}{i+1}}&0&\dots&0
\end{matrix}\right)=
\left(
\begin{matrix}
\frac{i+2}{i+1} & -1 & \dots & 0 \\
-1 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & -1\\
0 & \dots & -1 & 2
\end{matrix} \right)\]
So that we can get 
\[
U_{N}(i+1,i+1) = \sqrt{\frac{i+2}{i+1}}\]
\[
U_{N}(i+1,i+2) = \frac{B_{N}(i+1,i+2)}{U_{N}(i+1,i+1)} = -\sqrt{\frac{i+1}{i+2}}\]
So the induction has been proved. \\
Thus
\[
U_{N}(i,i) = \sqrt{\frac{i+1}{i}}, \forall i=1:N\]
\[
U_{N}(i,i+1) = -\sqrt{\frac{i}{i+1}}, \forall i=1:N-1\]\\

\end{homeworkProblem}





\begin{homeworkProblem}

Chapter 7 Problem 19 Finished by Xinlu Xiao\\
Covariance matrix is symmetric positive semidefinite matrix, and we can calculate all the leading principal minors of the matrix, we can get
\[
det\left(
\begin{matrix}
1
\end{matrix}\right) = 1 >0\]
\[
det\left(
\begin{matrix}
1 & 1\\
1 & 4
\end{matrix}\right) = 3>0\]
\[
det\left(
\begin{matrix}
1 & 1 & 0.5\\
1 & 4 & -2\\
0.5 & -2 & 9
\end{matrix}\right) = 20>0\]
So the covariance matrix is symmetric positive definite matrix, it has Cholesky decompositon, we can get
\[
U = \left(
\begin{matrix}
1 & 1 & 0.5\\
0 & 1.7321 & -1.4434\\
0 & 0 & 2.5820
\end{matrix}\right)\]
\[ 
U^{t} = \left(
\begin{matrix}
1 & 0 & 0\\
1 & 1.7321 & 0\\
0.5 & -1.4434 & 2.5820
\end{matrix}\right)
\]
So that the three normal random variables given by
\[
\left(
\begin{matrix}
X_{1}\\
X_{2}\\
X_{3}
\end{matrix}\right) = U^{t}
\left(
\begin{matrix}
Z_{1}\\
Z_{2}\\
Z_{3}
\end{matrix}\right) = 
\left(
\begin{matrix}
1 & 0 & 0\\
1 & 1.7321 & 0\\
0.5 & -1.4434 & 2.5820
\end{matrix}\right)
\left(
\begin{matrix}
Z_{1}\\
Z_{2}\\
Z_{3}
\end{matrix}\right) = 
\left(
\begin{matrix}
Z_{1} \\
Z_{1}+ 1.7321 Z_{2}\\
0.5Z_{1} -1.4434 Z_{2} + 2.582 Z_{3}
\end{matrix}\right) \]
















\end{homeworkProblem}

\end{document}